import pandas as pd ,os ,time ,requests ,warnings
from datasets import load_dataset
from huggingface_hub import list_datasets, HfApi, snapshot_download
from typing import Optional, List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®å‚æ•°
TIMEOUT = 30  # ç§’
MAX_RETRIES = 3
RETRY_DELAY = 5.0  # åŸºç¡€é‡è¯•é—´éš”
MAX_WORKERS = 2  # æœ€å¤§å¹¶å‘æ•°

# è®¾ç½®ä»£ç†ï¼ˆå¯æŒ‰éœ€æ³¨é‡Šï¼‰
# os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

def robust_operation(operation, *args, **kwargs):
    """é€šç”¨é‡è¯•è£…é¥°å™¨"""
    last_error = None
    for attempt in range(MAX_RETRIES + 1):
        try:
            return operation(*args, **kwargs)
        except Exception as e:
            last_error = e
            if attempt < MAX_RETRIES:
                wait_time = RETRY_DELAY * (attempt + 1)
                print(f"âš ï¸ æ“ä½œå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}), {wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
    raise last_error

def search_datasets(keyword: str, top_k: int = 5) -> list:
    """æœç´¢åŒ…å«å…³é”®è¯çš„æ•°æ®é›†ï¼ˆå¸¦è¶…æ—¶å’Œé‡è¯•ï¼‰"""
    print(f"ğŸ” æ­£åœ¨æœç´¢æ•°æ®é›† '{keyword}'...")
    api = HfApi()
    
    def _search():
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future = executor.submit(
                api.list_datasets,
                search=keyword,
                limit=top_k*2
            )
            return future.result(timeout=TIMEOUT)
    
    try:
        datasets = robust_operation(_search)
        matched = []
        seen = set()
        for ds in datasets:
            if keyword.lower() in ds.id.lower() and ds.id not in seen:
                seen.add(ds.id)
                matched.append(ds.id)
            if len(matched) >= top_k:
                break
        return matched[:top_k]
    except Exception as e:
        raise Exception(f"æœç´¢å¤±è´¥: {str(e)}")

def download_dataset(
    dataset_name: str,
    output_dir: str = "./data",
    download_all: bool = True,
    save_format: Optional[str] = None,
    sample: Optional[int] = None
) -> Dict:
    """
    ä¼˜åŒ–æ˜¾ç¤ºç‰ˆæœ¬çš„æ•°æ®é›†ä¸‹è½½
    """
    start_time = time.time()
    os.makedirs(output_dir, exist_ok=True)
    result = {"saved_files": []}

    try:
        # ä¸‹è½½åŸå§‹æ–‡ä»¶ï¼ˆå¸¦é™é»˜é‡è¯•ï¼‰
        if download_all:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ•°æ®é›† [{dataset_name}]...", end="", flush=True)
            
            # ç¦ç”¨huggingfaceçš„è­¦å‘Š
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                repo_path = robust_operation(
                    snapshot_download,
                    repo_id=dataset_name,
                    repo_type="dataset",
                    local_dir=os.path.join(output_dir, dataset_name.replace('/', '_')),
                    resume_download=True,
                    max_workers=MAX_WORKERS,
                    allow_patterns=["*"],
                    ignore_patterns=["*.lock", "*.tmp"]
                )
            
            print("\râœ… ä¸‹è½½å®Œæˆ!" + " " * 30)  # æ¸…é™¤è¿›åº¦è¡Œ
            result["saved_files"].append(repo_path)

        # æ•°æ®è½¬æ¢
        if save_format:
            print(f"ğŸ”„ æ­£åœ¨è½¬æ¢æ ¼å¼ ({save_format.upper()})...", end="", flush=True)
            dataset = robust_operation(load_dataset, dataset_name)
            
            for split_name, data in dataset.items():
                df = data.to_pandas()
                if sample:
                    df = df.sample(min(sample, len(df)))
                
                filename = f"{dataset_name.replace('/', '_')}_{split_name}.{save_format}"
                save_path = os.path.join(output_dir, filename)
                
                save_func = {
                    "csv": lambda: df.to_csv(save_path, index=False),
                    "json": lambda: df.to_json(save_path, orient="records"),
                    "parquet": lambda: df.to_parquet(save_path)
                }.get(save_format)
                
                if save_func:
                    robust_operation(save_func)
                    result["saved_files"].append(save_path)
            
            print("\râœ… æ ¼å¼è½¬æ¢å®Œæˆ!" + " " * 30)

        result.update({
            "status": "success",
            "time_used": f"{time.time() - start_time:.1f}s"
        })
        
    except Exception as e:
        print("\râŒ æ“ä½œå¤±è´¥!" + " " * 30)  # æ¸…é™¤è¿›åº¦è¡Œ
        result.update({
            "status": "error",
            "message": str(e),
            "time_used": f"{time.time() - start_time:.1f}s"
        })
    
    return result


def interactive_download():
    """äº¤äº’å¼ä¸‹è½½æµç¨‹ï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n" + "="*50)
    print("ğŸ” Hugging Face æ•°æ®é›†ä¸‹è½½å·¥å…· (v2.1)")
    print("="*50)
    
    try:
        keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆå¦‚ 'news' æˆ– 'ag_news'ï¼‰ï¼š").strip()
        if not keyword:
            raise ValueError("æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º")
        
        matched_datasets = search_datasets(keyword)
        if not matched_datasets:
            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®é›†")
            return
        
        print("\nğŸ“‚ åŒ¹é…çš„æ•°æ®é›†ï¼š")
        for i, ds in enumerate(matched_datasets, 1):
            print(f"{i}. {ds}")
        
        choice = input(f"\nè¯·è¾“å…¥è¦ä¸‹è½½çš„ç¼–å· (1-{len(matched_datasets)})ï¼š")
        try:
            selected = matched_datasets[int(choice)-1]
        except (ValueError, IndexError):
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥æœ‰æ•ˆç¼–å·")
            return
        
        save_format = input("ä¿å­˜æ ¼å¼ (csv/json/parquetï¼Œé»˜è®¤csv)ï¼š").strip().lower() or "csv"
        if save_format not in ["csv", "json", "parquet"]:
            print("âŒ ä¸æ”¯æŒçš„æ ¼å¼ï¼Œå°†ä½¿ç”¨CSV")
            save_format = "csv"
        
        output_dir = input(f"ä¿å­˜ç›®å½• (é»˜è®¤ './data')ï¼š").strip() or "./data"
        sample = input("é‡‡æ ·æ¡æ•°ï¼ˆç•™ç©ºä¸‹è½½å…¨éƒ¨ï¼‰ï¼š").strip()
        sample = int(sample) if sample.isdigit() else None
        
        result = download_dataset(
            dataset_name=selected,
            save_format=save_format,
            output_dir=output_dir,
            sample=sample
        )
        
        if result["status"] == "error":
            print(f"\nâŒ ä¸‹è½½å¤±è´¥: {result['message']}")
        else:
            print("\nğŸ‰ æ“ä½œå®Œæˆï¼")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {result['saved_files']}")
            print(f"â±ï¸ æ€»è€—æ—¶: {result['time_used']}")
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    interactive_download()
